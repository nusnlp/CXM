local xlm_model = "xlm-roberta-large";
local hdim = 1024;
local max_seq_len = 256;

{
    "dataset_reader": {
        "type": "cxm_reader",
        "lazy": false,
        "tokenizer": {
            "type": "pretrained_transformer",
            "model_name": xlm_model,
            "max_length": max_seq_len
        }, 
        "token_indexers": {
              "xlm": {
              "type": "pretrained_transformer",
              "model_name": xlm_model,
              "max_length": max_seq_len
            }
        }
    },
    "iterator": {
        "type": "bucket",
        "batch_size": 2,
        "max_instances_in_memory": 1000,
        "biggest_batch_first": true
    },
    "model": {
        "type": "cxm_model",
        "dropout": 0.3,
        "initializer": {
            "regexes":[
                [".*linear_layers.*weight", {"type": "xavier_normal"}],
                [".*token_embedder_tokens._projection.*weight", {"type": "xavier_normal"}]
            ]
        },
        "encoder": {
            "type": "lstm",
            "bidirectional": true,
            "hidden_size": hdim/2,
            "input_size": 1024,
            "num_layers": 1
        },
        "qdep_henc_rnn": {
            "type": "lstm",
            "bidirectional": true,
            "hidden_size": hdim/2,
            "input_size": 2 * hdim,
            "num_layers": 1
        },
        "senc_self_attn": {
            "gate": {
                "activations": "linear",
                "dropout": 0.5,
                "hidden_dims": 2 * hdim,
                "input_dim": 2 * hdim,
                "num_layers": 1
            },
            "input_dim": hdim,
            "num_factor": 1,
            "projector": {
                "activations": "tanh",
                "dropout": 0.5,
                "hidden_dims": hdim,
                "input_dim": hdim,
                "num_layers": 1
            }
        },
        "attnpool": {
            "projector": {
                "input_dim": hdim,
                "num_layers": 1,
                "hidden_dims": 1,
                "activations": "linear",
                "dropout": 0.0
            }
        },
        "output_ffl": {
            "input_dim": hdim,
            "num_layers": 1,
            "hidden_dims": 3,
            "activations": "linear",
            "dropout": 0.0
        },
        "text_field_embedder": {
            "token_embedders": {
                "xlm": {
                    "type": "pretrained_transformer",
                    "model_name": xlm_model,
                    "max_length": max_seq_len
                }
            }
        }
    },
    "train_data_path": "/data/en/train_d.json",
    "validation_data_path": "/data/en/dev.json",
    "test_data_path": "/data/en/eval.json",
    "evaluate_on_test": false,
    "trainer": {
        "cuda_device": 0,
        "learning_rate_scheduler": {
            "type": "reduce_on_plateau",
            "factor": 0.5,
            "mode": "max",
            "patience": 3
        },
        "num_epochs": 10,
        "optimizer": {
            "type": "huggingface_adamw",
            "weight_decay": 0.01,
            "parameter_groups": [[["bias", "gamma", "beta"], {"weight_decay": 0}]],
            "lr": 1e-6
        },
        "patience": 3,
        "validation_metric": "+accuracy",
        "checkpointer":{
            "num_serialized_models_to_keep": 2
        }
    }
}
